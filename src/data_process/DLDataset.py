import pandas as pd
import torch
import json
from torch.utils.data import Dataset
from src.utils import config

class MyDataset(Dataset):
    def __init__(self, path, dictionary=None, max_length=128, tokenizer=None, word=False):
        super(MyDataset, self).__init__()

        self.data = pd.read_csv(path).dropna()

        with open(config.root_path + '/data/label2id.json', 'r') as f:
            self.label2id = json.load(f)

        self.data['category_id'] = self.data['label'].map(self.label2id)

        if not word:
            self.data['text'] = self.data['text'].apply(lambda x: " ".join("".join(x.split())))

        if tokenizer is not None:
            self.model_name = 'bert'
            self.tokenizer = tokenizer
        else:
            self.model_name = 'normal'
            self.tokenizer = dictionary

        self.max_length = max_length

    def __getitem__(self, i):

        data = self.data.iloc[i] # 第 i 行

        text = data['text']
        labels = int(data['category_id'])

        # text： 智能 电网 芯片 技术 应用 智能 电网 发展趋势 自主 可靠 可控 安全 稳定 运行 迫切需要 推动 芯片 国产化 进程 新 技术 发展 要求 电力设备 更加 小型化 微型化 实现 移动 作业 促使 芯片 高 安全 高 可靠 高 集成 低功耗 方向 快速 发展 总结 智能 电网 芯片 技术 应用 成效 更好 研究 开发 应用 智能 电网 专用 芯片 技术 编写 智能 电网 芯片 技术 应用 一书 本书 共 分为 7 章 包括 概述 智能 电网 芯片 关键技术 主控 芯片 技术 应用 通信 芯片 技术 应用 安全 芯片 技术 应用 射频 识别
        # labels： 1

        attention_mask, token_type_ids = [0], [0]

        if 'bert' in self.model_name:
            # 如果是 bert 类模型， 使用 tokenizer 的encode_plus方法处理数据
            text_dict = self.tokenizer.encode_plus(
                text,  # Sentence to encode.
                add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
                max_length=self.max_length,  # Pad & truncate all sentences.
                ad_to_max_length=True,
                return_attention_mask=True,  # Construct attn. masks.
                # return_tensors='pt',     # Return pytorch tensors.
            )

            input_ids, attention_mask, token_type_ids = text_dict['input_ids'], text_dict['attention_mask'], text_dict['token_type_ids']

            # input_ids: len: 187 [101, 3255, 5543, 4510, 5381, 5708, 4275, 2825, 3318, 2418, 4500, 3255, 5543, 4510, 5381, 1355, 2245, 6633, 1232, 5632, 712, 1377, 7479, 1377, 2971, 2128, 1059, 4937, 2137, 6817, 6121, 6833, 1147, 7444, 6206, 2972, 1220, 5708, 4275, 1744, 772, 1265, 6822, 4923, 3173, 2825, 3318, 1355, 2245, 6206, 3724, 4510, 1213, 6392, 1906, 3291, 1217, 2207, 1798, 1265, 2544, 1798, 1265, 2141, 4385, 4919, 1220, 868, 689, 914, 886, 5708, 4275, 7770, 2128, 1059, 7770, 1377, 7479, 7770, 7415, 2768, 856, 1216, 5450, 3175, 1403, 2571, 6862, 1355, 2245, 2600, 5310, 3255, 5543, 4510, 5381, 5708, 4275, 2825, 3318, 2418, 4500, 2768, 3126, 3291, 1962, 4777, 4955, 2458, 1355, 2418, 4500, 3255, 5543, 4510, 5381, 683, 4500, 5708, 4275, 2825, 3318, 5356, 1091, 3255, 5543, 4510, 5381, 5708, 4275, 2825, 3318, 2418, 4500, 671, 741, 3315, 741, 1066, 1146, 711, 128, 4995, 1259, 2886, 3519, 6835, 3255, 5543, 4510, 5381, 5708, 4275, 1068, 7241, 2825, 3318, 712, 2971, 5708, 4275, 2825, 3318, 2418, 4500, 6858, 928, 5708, 4275, 2825, 3318, 2418, 4500, 2128, 1059, 5708, 4275, 2825, 3318, 2418, 4500, 2198, 7574, 6399, 1166, 102]
            # attention_mask: len: 187 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
            # token_type_ids: len: 187 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

        else:

            # 如果是 cnn rnn， transformer 则使用自建的dictionary 来处理
            text = text.split()
            text = text + [0] * max(0, self.max_length - len(text)) if len(text) < self.max_length else text[:self.max_length]
            input_ids = [self.tokenizer.indexer(x) for x in text]

        output = {
            "token_ids": input_ids,
            'attention_mask': attention_mask,
            "token_type_ids": token_type_ids,
            "labels": labels
        }
        return output

    def __len__(self):
        return self.data.shape[0]


def collate_fn(batch):
    """
    动态 padding: batch 是当前 batch 所有组的数据，如下所示：计算出当前 batch 所有组中最长的句子长度记为 max_length
    如果该 batch 中其他组数据的句子长度小于 max_length ，则进行 padding ，补零
     [{'token_ids': [101, 3152, 2422, 741, 7368, 7270, 3736, 3837, 1818, 3152, 3209, 3975, 6823, 3837, 7270, 704, 1744, 753, 1283, 1914, 2399, 3152, 1265, 6084, 1394, 704, 3152, 2422, 2347, 2768, 704, 1290, 3696, 3184, 3152, 1265, 6496, 2519, 2769, 1744, 1367, 807, 2456, 5029, 5102, 1798, 704, 1838, 4917, 4960, 1139, 671, 4905, 2769, 1744, 1367, 807, 3152, 1265, 6890, 772, 704, 3353, 1071, 7028, 6206, 5299, 2768, 6956, 1146, 741, 7368, 704, 1744, 3152, 1265, 1380, 677, 808, 782, 6614, 1386, 836, 1920, 1158, 715, 1044, 6777, 3255, 2716, 5310, 3253, 3152, 1265, 3136, 5509, 1501, 1456, 3354, 2682, 924, 2898, 3926, 7599, 3306, 3306, 3152, 1265, 4415, 2682, 5016, 1394, 704, 1744, 1744, 2658, 686, 782, 4917, 6887, 671, 3221, 704, 1744, 697, 1283, 2399, 3152, 1265, 837, 6853, 4777, 4955, 1216, 679, 1377, 3766, 753, 3221, 1824, 1075, 782, 812, 6614, 1386, 679, 2347, 3152, 1265, 5125, 5739, 6774, 1744, 2128, 6930, 3406, 3448, 722, 2798, 676, 3221, 1166, 1072, 671, 3419, 3136, 5509, 860, 1169, 102],
     'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
     'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 32}, {'token_ids': [101, 8138, 2399, 704, 1744, 2590, 2682, 7390, 5011, 2961, 6121, 3528, 4374, 2553, 5526, 712, 5356, 8138, 2399, 704, 1744, 2590, 2682, 7390, 5011, 2961, 6121, 3528, 5125, 6848, 8138, 2399, 7313, 1744, 1079, 831, 4899, 2590, 2682, 7390, 5011, 868, 5442, 5125, 1501, 1213, 868, 1066, 6369, 753, 1282, 1063, 5063, 1259, 2886, 3330, 1744, 3152, 3152, 782, 7270, 4764, 3448, 6130, 3152, 4995, 1920, 2157, 3688, 3813, 691, 4374, 5885, 6239, 2242, 3198, 807, 2552, 3255, 4135, 7410, 5384, 4458, 4447, 2548, 1744, 3325, 2697, 794, 5335, 4224, 814, 3307, 2590, 2682, 5442, 1155, 1086, 1908, 5811, 6241, 749, 679, 6629, 4905, 4905, 3152, 4995, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
     'labels': 0},
     { ... }
     ]

     "该函数对 bert 系列的模型数据加载时使用"
    """
    def padding(indice, max_length, pad_idx=0):
        """
        pad 函数
        注意 token type id 右侧 pad 添加 0
        """
        pad_indice = [
            item + [pad_idx] * max(0, max_length - len(item))
            for item in indice
        ]
        return torch.tensor(pad_indice)

    token_ids = [data["token_ids"] for data in batch]
    max_length = max([len(t) for t in token_ids]) # 取最长的句子最为当前 batch 的目标进行 padding
    token_type_ids = [data["token_type_ids"] for data in batch]
    attention_mask = [data["attention_mask"] for data in batch]
    labels = torch.tensor([data["labels"] for data in batch])
    token_ids_padded = padding(token_ids, max_length)
    token_type_ids_padded = padding(token_type_ids, max_length)
    attention_mask_padded = padding(attention_mask, max_length)
    #     print(token_ids_padded)
    return token_ids_padded, attention_mask_padded, token_type_ids_padded, labels